- [Link](https://arxiv.org/abs/2307.00184)
- Authors: [[(A) Greg Seapio-Garcia]] [[(A) Maja Mataric]]
- Topics: [[agents]] [[persona]]
- Venue: #arxiv
- Year: #y2023
- Read: 2024-05-28

---
### Major Contributions

1) [[personality]] measurements in the outputs of some LLMs under specific prompting configurations are reliable and valid
2) evidence of reliability and validity of synthetic LLM personality is stronger for larger and instruction fine-tuned models
3) personality in LLM outputs can be shaped along desired dimensions to mimic specific human personality profiles

---
### Secondary Contribution

- developed a structured prompting method that simulates persona descriptions
- novel prompting methodology that shapes personality traits at nine levels using 104 trait adjective

---
### Limitations/Future Work

- They provided a very extensive list of adjectives, but an ablation study would be nice. Are the small differences in wording that relevant?
- monocultural bias due to training data from EU and America
- the work is maybe biased towards psychometric test selection
- LLMs scored continuations instead of generating it. They are not specifically trained to do that.

---
### Notes (Try to use backlinks)

- Personality: They build on BigFive domains, expanded IPIP-NEO adjectives, and likert-scale like adverbs like "very", "extremely"
- Construct Validity:
	- Discriminant validity refers to how sufficiently unrelated a test is to indicators of unrelated constructs.
	- Criterion validity indicates how well a test relates to theoretically-linked external outcomes.
	- Convergent validity when it sufficiently relates to purported indicators of the test’s target construct.
	- Reliability refers to the consistency and dependability of a test’s measurements.

---
### Important Figures

![[2023_personality_traits_in_llms.png]]![[2023_personality_trait_markers.png]]